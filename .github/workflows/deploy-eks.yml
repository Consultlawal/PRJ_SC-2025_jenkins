name: new Deploy App & Security Mesh to Existing EKS

on:
  workflow_dispatch: # Allows manual trigger from GitHub UI
    inputs:
      cluster_name:
        description: 'Name of the existing EKS cluster'
        required: true
        default: 'terraform-eks-demo' # Set your default EKS cluster name here
      aws_region:
        description: 'AWS Region where the EKS cluster exists'
        required: true
        default: 'us-east-1' # Set your default AWS region here
  push:
    branches:
      - main # Triggers on push to the main branch

permissions:
  id-token: write # Required for OIDC authentication with AWS
  contents: read # Allows checkout of the repository content
  security-events: write # Allows uploading SARIF security reports if you integrate later

env:
  AWS_REGION: ${{ github.event.inputs.aws_region || 'us-east-1' }} # Fallback if not from workflow_dispatch
  CLUSTER_NAME: ${{ github.event.inputs.cluster_name || 'terraform-eks-demo' }} # Fallback if not from workflow_dispatch
  REPORTS_DIR: security-reports # Directory for storing all reports

jobs:
  deploy_app_and_security:
    name: Deploy Application and Security Mesh
    runs-on: ubuntu-latest # Uses the latest Ubuntu GitHub-hosted runner

    steps:
      # =======================================================
      # 1. SETUP & AUTHENTICATION
      # =======================================================
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # THIS IS CRITICAL: The ARN of the IAM Role for GitHub Actions
          role-to-assume: arn:aws:iam::009593259890:role/github-actions-terraform-role
          aws-region: us-east-1

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.0'

      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      # =======================================================
      # 2. PREPARE KUBERNETES CONTEXT (Connect to your existing EKS)
      # =======================================================
      - name: Update Kubeconfig
        run: |
          mkdir -p $HOME/.kube/
          aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          echo "Kubeconfig updated for cluster ${{ env.CLUSTER_NAME }}."
          kubectl get nodes # Verify kubectl can connect to your EKS cluster

      - name: Create Reports Directory
        run: mkdir -p ${{ env.REPORTS_DIR }}

      - name: Check API Server Responsiveness
        run: |
          echo "Testing Kubernetes API server latency..."
          time kubectl get --raw /healthz
          time kubectl get nodes

      # =======================================================
      # 3. SETUP ISTIO (Now that Kubeconfig is ready)
      # =======================================================
      - name: Setup Istioctl CLI
        run: |
          echo "Setting up istioctl version 1.20.0..."
          # The curl command output is captured and then echoed to avoid issues
          # if the output itself contains special characters that confuse the shell.
          ISTIO_INSTALL_OUTPUT=$(curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.0 sh - 2>&1)
          echo "$ISTIO_INSTALL_OUTPUT" # Log the output for visibility

          ISTIO_BIN_DIR="${{ github.workspace }}/istio-1.20.0/bin"
          export PATH="$PATH:$ISTIO_BIN_DIR" # For current run step
          echo "$ISTIO_BIN_DIR" >> $GITHUB_PATH # For subsequent steps

          istioctl version --remote=false # This should pass
          echo "Istioctl setup complete."

          echo "Running Istio pre-installation check..."
          istioctl x precheck # This should now succeed

      # =======================================================
      # 4. DEPLOY SECURITY MESH COMPONENTS (AND OPTIONAL CORE ADDONS)
      # =======================================================
      - name: Uninstall previous deployments (Cleanup)
        run: |
          helm uninstall falco -n falco || true
          helm uninstall trivy-operator -n trivy-system || true # Added trivy-operator for clean slate
          helm uninstall kyverno -n kyverno || true
          helm uninstall kube-prometheus-stack -n monitoring || true
          # Add any other large chart uninstalls here if needed later (e.g., Istio)
          echo "Waiting 30 seconds for resources to terminate..."
          sleep 30
          # Optionally delete namespaces, but be careful with core ones like kube-system or default
          # kubectl delete ns falco || true
          # kubectl delete ns trivy-system || true
          # kubectl delete ns kyverno || true
          # kubectl delete ns monitoring || true

      - name: Install Kyverno (Policy Engine)
        run: |
          helm repo add kyverno https://kyverno.github.io/kyverno/ --force-update
          helm upgrade --install kyverno kyverno/kyverno -n kyverno --create-namespace --wait --atomic \
          --timeout=15m0s
          echo "Kyverno installation complete."

      - name: Apply Kyverno Policies
        run: |
          if [ -d "k8s-policies/kyverno" ]; then
            echo "Applying Kyverno policies from k8s-policies/kyverno/"
            kubectl apply -f k8s-policies/kyverno/
            sleep 10 # Give Kyverno a moment to register policies
            echo "Kyverno policies applied."
          else
            echo "::warning:: k8s-policies/kyverno/ directory not found. Skipping Kyverno policy application."
          fi

      # ------------------ CREATE FALCO VALUES FILE ------------------
      - name: Create Falco values file
        run: |
          mkdir -p falco/
          cat <<'EOF' > falco/falco-values.yaml
          daemonset:
            podLabels:
              app: falco
              environment: production

          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi

          driver:
            enabled: true
            loader:
              initContainer:
                enabled: true
                image:
                  registry: docker.io
                  repository: falcosecurity/falco-driver-loader
                  tag: latest
                  pullPolicy: IfNotPresent
                resources:
                  limits:
                    cpu: 200m
                    memory: 256Mi
                  requests:
                    cpu: 100m
                    memory: 128Mi

          falcoctl:
            artifact:
              install:
                resources:
                  limits:
                    cpu: 200m
                    memory: 256Mi
                  requests:
                    cpu: 100m
                    memory: 128Mi

          falcoExporter:
            enabled: true
            resources:
              limits:
                cpu: 200m
                memory: 256Mi
              requests:
                cpu: 100m
                memory: 128Mi
          EOF
          echo "Falco values file created."

      # ------------------ INSTALL FALCO ------------------
      - name: Install Falco (Runtime Security Engine)
        run: |
          helm repo add falcosecurity https://falcosecurity.github.io/charts --force-update
          helm upgrade --install falco falcosecurity/falco \
            -n falco --create-namespace --wait --atomic \
            -f falco/falco-values.yaml \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ secrets.FALCO_IRSA_ROLE_ARN }}" \
            --timeout=10m0s # Added timeout for Falco
          echo "Falco installation complete."

      # ------------------ DIAGNOSTICS (keep this step) ------------------
      - name: Diagnose Falco Installation Failure
        if: failure()
        run: |
          echo "--- Falco Installation Diagnostics ---"

          echo "Current context:"
          kubectl config current-context

          echo "Falco namespace:"
          kubectl get namespace falco || true

          echo "Falco DaemonSet:"
          kubectl get daemonset -n falco falco || true
          kubectl describe daemonset -n falco falco || true

          echo "Falco Pods:"
          kubectl get pods -n falco -o wide || true
          kubectl describe pods -l app.kubernetes.io/name=falco -n falco || true

          echo "Falco pod logs:"
          P=$(kubectl get pods -n falco -l app.kubernetes.io/name=falco -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          if [ -n "$P" ]; then
            kubectl logs "$P" -n falco --all-containers || true
          fi

          echo "Namespace events:"
          kubectl get events -n falco --sort-by='.lastTimestamp' || true

          echo "--- END DIAGNOSTICS ---"

      - name: Install Trivy Operator (Vulnerability Scanner)
        run: |
          helm repo add aqua https://aquasecurity.github.io/helm-charts --force-update
          helm upgrade --install trivy-operator aqua/trivy-operator -n trivy-system --create-namespace --wait --atomic \
            --timeout=10m0s # Added timeout for Trivy Operator
          echo "Trivy Operator installation complete."

      - name: Install Istio Service Mesh (Demo Profile)
        run: |
          istioctl install --set profile=demo -y
          echo "Istio Service Mesh installed with the demo profile."

      # - name: Install Prometheus and Grafana (Monitoring Stack)
      #   run: |
      #     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
      #     helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
      #       --version 47.6.0 \
      #       --namespace monitoring \
      #       --create-namespace \
      #       -f k8s-policies/monitoring/monitoring-values.yaml \
      #       --wait \
      #       --atomic \
      #       --timeout=20m0s # Corrected line continuation and set to 20m
      #     echo "Prometheus and Grafana installation complete."

      # - name: Install Prometheus and Grafana (Monitoring Stack)
      #   run: |
      #     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
      #     helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
      #       --version 47.6.0 \
      #       --namespace monitoring \
      #       --create-namespace \
      #       -f k8s-policies/monitoring/monitoring-values.yaml \
      #       --wait \
      #       --timeout=20m0s
      #       # Removed --atomic so resources stay on failure for debugging
      #     echo "Prometheus and Grafana installation complete."

      # - name: Install Prometheus and Grafana (Monitoring Stack)
      #   env:
      #     KUBECONFIG_QPS: "20"
      #     KUBECONFIG_BURST: "50"
      #   run: |
      #     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
      #     helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
      #       --version 47.6.0 \
      #       --namespace monitoring \
      #       --create-namespace \
      #       -f k8s-policies/monitoring/monitoring-values.yaml \
      #       --wait \
      #       --atomic \
      #       --timeout=20m0s
      - name: Install Prometheus and Grafana (Monitoring Stack)
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --version 47.6.0 \
            --namespace monitoring \
            --create-namespace \
            -f k8s-policies/monitoring/monitoring-values.yaml \
            --wait \
            --atomic \
            --timeout=20m0s
          echo "Prometheus and Grafana installation complete."

      - name: Diagnose Prometheus Installation Failure
        if: failure()
        run: |
          echo "--- Prometheus Stack Installation Diagnostics ---"

          echo "Current context:"
          kubectl config current-context

          echo "Monitoring namespace:"
          kubectl get namespace monitoring || true

          echo "Prometheus Stack resources in monitoring namespace:"
          kubectl get all -n monitoring || true

          echo "Prometheus Operator deployment status:"
          kubectl get deployment prometheus-operator -n monitoring || true
          kubectl describe deployment prometheus-operator -n monitoring || true

          echo "Prometheus Operator pod logs:"
          PROMETHEUS_OPERATOR_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus-operator -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          if [ -n "$PROMETHEUS_OPERATOR_POD" ]; then
            echo "Found Prometheus Operator Pod: $PROMETHEUS_OPERATOR_POD"
            kubectl logs "$PROMETHEUS_OPERATOR_POD" -n monitoring --tail=100 || true
          else
            echo "::warning:: Prometheus Operator pod not found."
          fi

          echo "Prometheus (main instance) deployment/statefulset status:"
          kubectl get prometheus -n monitoring || true # Custom Resource
          kubectl get statefulset prometheus-kube-prometheus-stack-prometheus -n monitoring || true
          kubectl describe statefulset prometheus-kube-prometheus-stack-prometheus -n monitoring || true

          echo "Grafana deployment status:"
          kubectl get deployment kube-prometheus-stack-grafana -n monitoring || true
          kubectl describe deployment kube-prometheus-stack-grafana -n monitoring || true

          echo "Namespace events for monitoring:"
          kubectl get events -n monitoring --sort-by='.lastTimestamp' || true

          echo "All pods in monitoring namespace (detailed):"
          kubectl get pods -n monitoring -o wide || true
          kubectl describe pods -l release=kube-prometheus-stack -n monitoring || true

          echo "MutatingWebhookConfigurations and ValidatingWebhookConfigurations (cluster-wide):"
          kubectl get mutatingwebhookconfigurations -o yaml | grep -A2 -E "name|monitoring" || true
          kubectl get validatingwebhookconfigurations -o yaml | grep -A2 -E "name|monitoring" || true

          echo "--- END PROMETHEUS STACK DIAGNOSTICS ---"
      # =======================================================
      # 5. DEPLOY APPLICATION (Hipster Shop & Route 53 Ingress)
      # =======================================================
      - name: Deploy Hipster Shop and Ingress
        run: |
          echo "Creating namespace..."
          kubectl create ns hipster-shop --dry-run=client -o yaml | kubectl apply -f -
          kubectl label namespace hipster-shop istio-injection=enabled --overwrite
          echo "Namespace 'hipster-shop' created and Istio injection enabled."

          echo "Adding Kyverno exception for Hipster Shop..."
          cat <<EOF | kubectl apply -f -
          apiVersion: kyverno.io/v2
          kind: PolicyException
          metadata:
            name: hipster-shop-exception
            namespace: hipster-shop
          spec:
            exceptions:
              - policyName: enforce-labels
                ruleNames:
                  - require-app-and-env-labels
              - policyName: restrict-image-registries
                ruleNames:
                  - validate-registry
            match:
              any:
                - resources:
                    kinds:
                      - Deployment
                      - Pod
                    namespaces:
                      - hipster-shop
          EOF
              echo "Kyverno exception applied."

              echo "Deploying Hipster Shop application..."
              kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml -n hipster-shop
              echo "Hipster Shop application manifests applied."

              echo "Deploying Hipster Shop Ingress..."
              kubectl apply -f ./k8s/hipster-shop-ingress.yaml -n hipster-shop
              echo "Hipster Shop Ingress applied."

              echo "Waiting for Hipster Shop frontend to be ready (max 5 minutes)..."
              kubectl wait --for=condition=Available deployment/frontend -n hipster-shop --timeout=5m
              echo "Hipster Shop frontend deployment is ready."

              echo "Configuring Route53 DNS for frontend.ailawal.ca..."
              ALB_DNS=$(kubectl get svc -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
              echo "ALB DNS: $ALB_DNS"

              aws route53 change-resource-record-sets \
                --hosted-zone-id Z097297112UWA30TUHJXW \
                --change-batch "{
                  \"Changes\": [{
                    \"Action\": \"UPSERT\",
                    \"ResourceRecordSet\": {
                      \"Name\": \"frontend.ailawal.ca\",
                      \"Type\": \"CNAME\",
                      \"TTL\": 300,
                      \"ResourceRecords\": [{\"Value\": \"$ALB_DNS\"}]
                    }
                  }]
                }"
              echo "DNS record for frontend.ailawal.ca has been updated."


      # =======================================================
      # 6. RUN SECURITY AUDITS & REPORTING
      # =======================================================
      - name: Audit Calico Network Policies
        run: |
          echo "[+] Exporting Calico Network Policies..."
          if kubectl get networkpolicies -A &>/dev/null; then
            kubectl get networkpolicies -A -o json > ${{ env.REPORTS_DIR }}/calico-networkpolicies.json
          else
            echo "::warning:: No network policies found or Calico not installed. Skipping Calico Network Policies audit."
          fi

      - name: Audit Istio Security Configurations
        run: |
          echo "[+] Exporting Istio Security Configurations..."
          if kubectl get peerauthentication -A &>/dev/null; then
            kubectl get peerauthentication -A -o json > ${{ env.REPORTS_DIR }}/istio-peerauth.json
          else
            echo "::warning:: No PeerAuthentication resources found. Skipping Istio PeerAuthentication audit."
          fi
          if kubectl get authorizationpolicy -A &>/dev/null; then
            kubectl get authorizationpolicy -A -o json > ${{ env.REPORTS_DIR }}/istio-authz.json
          else
            echo "::warning:: No AuthorizationPolicy resources found. Skipping Istio AuthorizationPolicy audit."
          fi

      - name: Run kube-bench (CIS Benchmark)
        run: |
          echo "[+] Running kube-bench CIS Benchmark scan inside the cluster..."
          kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml -n kube-system

          echo "--- Kube-bench Diagnostics ---"
          echo "Getting Kube-bench Job status (in kube-system):"
          kubectl get job kube-bench -n kube-system || true
          echo "Getting Kube-bench Pods (in kube-system):"
          kubectl get pods -l app=kube-bench -n kube-system || true

          KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -n kube-system -o jsonpath='{.items[0].metadata.name}' || true)

          if [ -n "$KUBE_BENCH_POD" ]; then
            echo "Found Kube-bench Pod: $KUBE_BENCH_POD"
            echo "Describing Kube-bench Pod:"
            kubectl describe pod "$KUBE_BENCH_POD" -n kube-system || true
            echo "Getting Kube-bench Pod logs:"
            kubectl logs "$KUBE_BENCH_POD" -n kube-system || true
          else
            echo "::warning:: Kube-bench pod not found in 'kube-system' namespace (or it already succeeded/failed and was cleaned up before being found by this script)."
            echo "Attempting to list all pods in 'kube-system' namespace for more context:"
            kubectl get pods -n kube-system || true
          fi
          echo "--- End Kube-bench Diagnostics ---"

          echo "Waiting for kube-bench job to complete (max 10 minutes)..."
          kubectl wait --for=condition=complete job/kube-bench -n kube-system --timeout=10m

          KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -n kube-system -o jsonpath='{.items[0].metadata.name}' || true)
          if [ -n "$KUBE_BENCH_POD" ]; then
            echo "Fetching logs from kube-bench pod: $KUBE_BENCH_POD"
            kubectl logs "$KUBE_BENCH_POD" -n kube-system > ${{ env.REPORTS_DIR }}/kube-bench-report.txt
            echo "Deleting kube-bench job..."
            kubectl delete -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml -n kube-system
            echo "[+] kube-bench report saved and job cleaned up."
          else
            echo "::warning:: Kube-bench pod not found. Skipping report collection and cleanup."
          fi

      - name: Fetch Falco Alerts
        run: |
          echo "[+] Fetching Falco runtime security alerts..."
          if kubectl get pods -l app.kubernetes.io/name=falco -n falco &>/dev/null; then
            kubectl logs -l app.kubernetes.io/name=falco -n falco --all-containers --since=1h > ${{ env.REPORTS_DIR }}/falco-alerts.log || true
            echo "[+] Falco alerts exported."
          else
            echo "::warning:: No Falco pods found in 'falco' namespace. Skipping Falco alerts export."
          fi

      - name: Check Trivy Operator Vulnerability Reports
        run: |
          echo "[+] Checking Trivy Operator vulnerability reports..."
          if kubectl get vulnerabilityreports -A &>/dev/null; then
            kubectl get vulnerabilityreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-vulnerabilityreports.yaml
          else
            echo "::warning:: No vulnerabilityreports found. Skipping Trivy Operator vulnerability reports export."
          fi
          if kubectl get configauditreports -A &>/dev/null; then
            kubectl get configauditreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-configauditreports.yaml
          else
            echo "::warning:: No configauditreports found. Skipping Trivy Operator config audit reports export."
          fi
          echo "[+] Trivy Operator reports exported."

      - name: Check Kyverno Policy Reports
        run: |
          echo "[+] Checking Kyverno policy reports..."
          if kubectl get policyreports -A &>/dev/null; then
            kubectl get policyreports -A -o yaml > ${{ env.REPORTS_DIR }}/kyverno-policyreports.yaml
          else
            echo "::warning:: No policyreports found. Skipping Kyverno policy reports export."
          fi
          echo "[+] Kyverno policy reports exported."

      # =======================================================
      # 7. UPLOAD REPORTS AS ARTIFACTS
      # =======================================================
      - name: Upload Security Reports Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: security-reports-${{ github.run_id }}
          path: ${{ env.REPORTS_DIR }}/
          retention-days: 7
          if-no-files-found: ignore



# name: new Deploy App & Security Mesh to Existing EKS

# on:
#   workflow_dispatch: # Allows manual trigger from GitHub UI
#     inputs:
#       cluster_name:
#         description: 'Name of the existing EKS cluster'
#         required: true
#         default: 'terraform-eks-demo' # Set your default EKS cluster name here
#       aws_region:
#         description: 'AWS Region where the EKS cluster exists'
#         required: true
#         default: 'us-east-1' # Set your default AWS region here
#   push:
#     branches:
#       - main # Triggers on push to the main branch

# permissions:
#   id-token: write # Required for OIDC authentication with AWS
#   contents: read # Allows checkout of the repository content
#   security-events: write # Allows uploading SARIF security reports if you integrate later

# env:
#   AWS_REGION: ${{ github.event.inputs.aws_region || 'us-east-1' }} # Fallback if not from workflow_dispatch
#   CLUSTER_NAME: ${{ github.event.inputs.cluster_name || 'terraform-eks-demo' }} # Fallback if not from workflow_dispatch
#   REPORTS_DIR: security-reports # Directory for storing all reports

# jobs:
#   deploy_app_and_security:
#     name: Deploy Application and Security Mesh
#     runs-on: ubuntu-latest # Uses the latest Ubuntu GitHub-hosted runner

#     steps:
#       # =======================================================
#       # 1. SETUP & AUTHENTICATION
#       # =======================================================
#       - name: Checkout Code
#         uses: actions/checkout@v4

#       - name: Configure AWS Credentials
#         uses: aws-actions/configure-aws-credentials@v4
#         with:
#           # THIS IS CRITICAL: The ARN of the IAM Role for GitHub Actions
#           # role-to-assume: ${{ secrets.GH_ACTIONS_ROLE_ARN }} # <-- Use the secret
#           # aws-region: ${{ env.AWS_REGION }}
#           # The hardcoded ARN below should NOT be used in production.
#           role-to-assume: arn:aws:iam::009593259890:role/github-actions-terraform-role
#           aws-region: us-east-1

#       - name: Setup kubectl
#         uses: azure/setup-kubectl@v3
#         with:
#           version: 'v1.29.0'

#       - name: Setup Helm
#         uses: azure/setup-helm@v3
#         with:
#           version: 'v3.12.0'

#       # =======================================================
#       # 2. PREPARE KUBERNETES CONTEXT (Connect to your existing EKS)
#       # =======================================================
#       - name: Update Kubeconfig
#         run: |
#           mkdir -p $HOME/.kube/
#           aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
#           echo "Kubeconfig updated for cluster ${{ env.CLUSTER_NAME }}."
#           kubectl get nodes # Verify kubectl can connect to your EKS cluster

#       - name: Create Reports Directory
#         run: mkdir -p ${{ env.REPORTS_DIR }}

#       # =======================================================
#       # 3. SETUP ISTIO (Now that Kubeconfig is ready)
#       # =======================================================
#       - name: Setup Istioctl CLI
#         run: |
#           echo "Setting up istioctl version 1.20.0..."
#           # The curl command output is captured and then echoed to avoid issues
#           # if the output itself contains special characters that confuse the shell.
#           ISTIO_INSTALL_OUTPUT=$(curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.0 sh - 2>&1)
#           echo "$ISTIO_INSTALL_OUTPUT" # Log the output for visibility

#           ISTIO_BIN_DIR="${{ github.workspace }}/istio-1.20.0/bin"
#           export PATH="$PATH:$ISTIO_BIN_DIR" # For current run step
#           echo "$ISTIO_BIN_DIR" >> $GITHUB_PATH # For subsequent steps

#           istioctl version --remote=false # This should pass
#           echo "Istioctl setup complete."

#           echo "Running Istio pre-installation check..."
#           istioctl x precheck # This should now succeed

#       # =======================================================
#       # 4. DEPLOY SECURITY MESH COMPONENTS (AND OPTIONAL CORE ADDONS)
#       # =======================================================
#       - name: Uninstall previous deployments (Cleanup)
#         run: |
#           helm uninstall falco -n falco || true
#           helm uninstall kyverno -n kyverno || true
#           # Wait a moment for resources to be deleted
#           sleep 30
#       - name: Install Kyverno (Policy Engine)
#         run: |
#           helm repo add kyverno https://kyverno.github.io/kyverno/ --force-update
#           helm upgrade --install kyverno kyverno/kyverno -n kyverno --create-namespace --wait --atomic \
#           --timeout=15m0s
#           echo "Kyverno installation complete."

#       - name: Apply Kyverno Policies
#         run: |
#           if [ -d "k8s-policies/kyverno" ]; then
#             echo "Applying Kyverno policies from k8s-policies/kyverno/"
#             kubectl apply -f k8s-policies/kyverno/
#             sleep 10 # Give Kyverno a moment to register policies
#             echo "Kyverno policies applied."
#           else
#             echo "::warning:: k8s-policies/kyverno/ directory not found. Skipping Kyverno policy application."
#           fi

#       # ------------------ CREATE VALUES FILE ------------------
#       - name: Create Falco values file
#         run: |
#           mkdir -p falco/
#           cat <<'EOF' > falco/falco-values.yaml
#           daemonset:
#             podLabels:
#               app: falco
#               environment: production

#           resources:
#             limits:
#               cpu: 500m
#               memory: 512Mi
#             requests:
#               cpu: 200m
#               memory: 256Mi

#           driver:
#             enabled: true
#             loader:
#               initContainer:
#                 enabled: true
#                 image:
#                   registry: docker.io
#                   repository: falcosecurity/falco-driver-loader
#                   tag: latest
#                   pullPolicy: IfNotPresent
#                 resources:
#                   limits:
#                     cpu: 200m
#                     memory: 256Mi
#                   requests:
#                     cpu: 100m
#                     memory: 128Mi

#           falcoctl:
#             artifact:
#               install:
#                 resources:
#                   limits:
#                     cpu: 200m
#                     memory: 256Mi
#                   requests:
#                     cpu: 100m
#                     memory: 128Mi

#           falcoExporter:
#             enabled: true
#             resources:
#               limits:
#                 cpu: 200m
#                 memory: 256Mi
#               requests:
#                 cpu: 100m
#                 memory: 128Mi
#           EOF
#           echo "Falco values file created."

#       # ------------------ INSTALL FALCO ------------------
#       - name: Install Falco (Runtime Security Engine)
#         run: |
#           helm repo add falcosecurity https://falcosecurity.github.io/charts --force-update
#           helm upgrade --install falco falcosecurity/falco \
#             -n falco --create-namespace --wait --atomic \
#             -f falco/falco-values.yaml \
#             --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ secrets.FALCO_IRSA_ROLE_ARN }}"
#           echo "Falco installation complete."

#       # ------------------ DIAGNOSTICS (keep this step) ------------------
#       - name: Diagnose Falco Installation Failure
#         if: failure()
#         run: |
#           echo "--- Falco Installation Diagnostics ---"

#           echo "Current context:"
#           kubectl config current-context

#           echo "Falco namespace:"
#           kubectl get namespace falco || true

#           echo "Falco DaemonSet:"
#           kubectl get daemonset -n falco falco || true
#           kubectl describe daemonset -n falco falco || true

#           echo "Falco Pods:"
#           kubectl get pods -n falco -o wide || true
#           kubectl describe pods -l app.kubernetes.io/name=falco -n falco || true

#           echo "Falco pod logs:"
#           P=$(kubectl get pods -n falco -l app.kubernetes.io/name=falco -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
#           if [ -n "$P" ]; then
#             kubectl logs "$P" -n falco --all-containers || true
#           fi

#           echo "Namespace events:"
#           kubectl get events -n falco --sort-by='.lastTimestamp' || true

#           echo "--- END DIAGNOSTICS ---"


#       - name: Install Trivy Operator (Vulnerability Scanner)
#         run: |
#           helm repo add aqua https://aquasecurity.github.io/helm-charts --force-update
#           helm upgrade --install trivy-operator aqua/trivy-operator -n trivy-system --create-namespace --wait --atomic
#           echo "Trivy Operator installation complete."

#       - name: Install Istio Service Mesh (Demo Profile)
#         run: |
#           istioctl install --set profile=demo -y
#           echo "Istio Service Mesh installed with the demo profile."

#       - name: Uninstall previous deployments (Cleanup)
#         run: |
#           helm uninstall falco -n falco || true
#           helm uninstall trivy-operator -n trivy-system || true # Adding trivy-operator for completeness
#           helm uninstall kyverno -n kyverno || true
#           helm uninstall kube-prometheus-stack -n monitoring || true # <--- ADD THIS LINE
#           # Add any other large chart uninstalls here if needed later (e.g., Istio)
#           echo "Waiting 30 seconds for resources to terminate..."
#           sleep 30
#           # Optionally delete namespaces, but be careful with core ones like kube-system or default
#           # kubectl delete ns falco || true
#           # kubectl delete ns trivy-system || true
#           # kubectl delete ns kyverno || true
#           # kubectl delete ns monitoring || true
#       - name: Install Prometheus and Grafana (Monitoring Stack)
#         run: |
#           helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
#           helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
#             --version 47.6.0 \
#             --namespace monitoring \
#             --create-namespace \
#             -f k8s-policies/monitoring/monitoring-values.yaml \
#             --wait \
#             --atomic \
#             --timeout=20m0s # Changed to 20m, as we discussed longer timeouts might be needed
#           echo "Prometheus and Grafana installation complete."

#       # =======================================================
#       # 5. DEPLOY APPLICATION (Hipster Shop & Route 53 Ingress)
#       # =======================================================
#       - name: Deploy Hipster Shop and Ingress
#         run: |
#           kubectl create ns hipster-shop --dry-run=client -o yaml | kubectl apply -f -
#           kubectl label namespace hipster-shop istio-injection=enabled --overwrite
#           echo "Namespace 'hipster-shop' created and Istio injection enabled."

#           echo "Deploying Hipster Shop application..."
#           kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml -n hipster-shop
#           echo "Hipster Shop application manifests applied."

#           echo "Deploying Hipster Shop Ingress..."
#           kubectl apply -f hipster-shop-ingress.yaml -n hipster-shop
#           echo "Hipster Shop Ingress applied."

#           echo "Waiting for Hipster Shop frontend to be ready (max 5 minutes)..."
#           kubectl wait --for=condition=Available deployment/frontend -n hipster-shop --timeout=5m
#           echo "Hipster Shop frontend deployment is ready."

#       # =======================================================
#       # 6. RUN SECURITY AUDITS & REPORTING
#       # =======================================================
#       - name: Audit Calico Network Policies
#         run: |
#           echo "[+] Exporting Calico Network Policies..."
#           if kubectl get networkpolicies -A &>/dev/null; then
#             kubectl get networkpolicies -A -o json > ${{ env.REPORTS_DIR }}/calico-networkpolicies.json
#           else
#             echo "::warning:: No network policies found or Calico not installed. Skipping Calico Network Policies audit."
#           fi

#       - name: Audit Istio Security Configurations
#         run: |
#           echo "[+] Exporting Istio Security Configurations..."
#           if kubectl get peerauthentication -A &>/dev/null; then
#             kubectl get peerauthentication -A -o json > ${{ env.REPORTS_DIR }}/istio-peerauth.json
#           else
#             echo "::warning:: No PeerAuthentication resources found. Skipping Istio PeerAuthentication audit."
#           fi
#           if kubectl get authorizationpolicy -A &>/dev/null; then
#             kubectl get authorizationpolicy -A -o json > ${{ env.REPORTS_DIR }}/istio-authz.json
#           else
#             echo "::warning:: No AuthorizationPolicy resources found. Skipping Istio AuthorizationPolicy audit."
#           fi

#       - name: Run kube-bench (CIS Benchmark)
#         run: |
#           echo "[+] Running kube-bench CIS Benchmark scan inside the cluster..."
#           kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml -n kube-system

#           # --- ADD THESE DIAGNOSTIC STEPS HERE ---
#           echo "--- Kube-bench Diagnostics ---"
#           echo "Getting Kube-bench Job status (in kube-system):"
#           kubectl get job kube-bench -n kube-system || true
#           echo "Getting Kube-bench Pods (in kube-system):"
#           kubectl get pods -l app=kube-bench -n kube-system || true

#           KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -n kube-system -o jsonpath='{.items[0].metadata.name}' || true)

#           if [ -n "$KUBE_BENCH_POD" ]; then
#             echo "Found Kube-bench Pod: $KUBE_BENCH_POD"
#             echo "Describing Kube-bench Pod:"
#             kubectl describe pod "$KUBE_BENCH_POD" -n kube-system || true
#             echo "Getting Kube-bench Pod logs:"
#             kubectl logs "$KUBE_BENCH_POD" -n kube-system || true
#           else
#             echo "::warning:: Kube-bench pod not found in 'kube-system' namespace (or it already succeeded/failed and was cleaned up before being found by this script)."
#             echo "Attempting to list all pods in 'kube-system' namespace for more context:"
#             kubectl get pods -n kube-system || true
#           fi
#           echo "--- End Kube-bench Diagnostics ---"
#           # --- END DIAGNOSTIC STEPS ---

#           echo "Waiting for kube-bench job to complete (max 10 minutes)..."
#           kubectl wait --for=condition=complete job/kube-bench -n kube-system --timeout=10m

#           KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -n kube-system -o jsonpath='{.items[0].metadata.name}' || true)
#           if [ -n "$KUBE_BENCH_POD" ]; then
#             echo "Fetching logs from kube-bench pod: $KUBE_BENCH_POD"
#             kubectl logs "$KUBE_BENCH_POD" -n kube-system > ${{ env.REPORTS_DIR }}/kube-bench-report.txt
#             echo "Deleting kube-bench job..."
#             kubectl delete -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml -n kube-system
#             echo "[+] kube-bench report saved and job cleaned up."
#           else
#             echo "::warning:: Kube-bench pod not found. Skipping report collection and cleanup."
#           fi

#           echo "Waiting for kube-bench job to complete (max 10 minutes)..."
#           kubectl wait --for=condition=complete job/kube-bench -n kube-system --timeout=10m

#           KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -n kube-system -o jsonpath='{.items[0].metadata.name}' || true)
#           if [ -n "$KUBE_BENCH_POD" ]; then
#             echo "Fetching logs from kube-bench pod: $KUBE_BENCH_POD"
#             kubectl logs "$KUBE_BENCH_POD" -n kube-system > ${{ env.REPORTS_DIR }}/kube-bench-report.txt
#             echo "Deleting kube-bench job..."
#             kubectl delete -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml -n kube-system
#             echo "[+] kube-bench report saved and job cleaned up."
#           else
#             echo "::warning:: Kube-bench pod not found. Skipping report collection and cleanup."
#           fi

#       - name: Fetch Falco Alerts
#         run: |
#           echo "[+] Fetching Falco runtime security alerts..."
#           if kubectl get pods -l app.kubernetes.io/name=falco -n falco &>/dev/null; then
#             kubectl logs -l app.kubernetes.io/name=falco -n falco --all-containers --since=1h > ${{ env.REPORTS_DIR }}/falco-alerts.log || true
#             echo "[+] Falco alerts exported."
#           else
#             echo "::warning:: No Falco pods found in 'falco' namespace. Skipping Falco alerts export."
#           fi

#       - name: Check Trivy Operator Vulnerability Reports
#         run: |
#           echo "[+] Checking Trivy Operator vulnerability reports..."
#           if kubectl get vulnerabilityreports -A &>/dev/null; then
#             kubectl get vulnerabilityreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-vulnerabilityreports.yaml
#           else
#             echo "::warning:: No vulnerabilityreports found. Skipping Trivy Operator vulnerability reports export."
#           fi
#           if kubectl get configauditreports -A &>/dev/null; then
#             kubectl get configauditreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-configauditreports.yaml
#           else
#             echo "::warning:: No configauditreports found. Skipping Trivy Operator config audit reports export."
#           fi
#           echo "[+] Trivy Operator reports exported."

#       - name: Check Kyverno Policy Reports
#         run: |
#           echo "[+] Checking Kyverno policy reports..."
#           if kubectl get policyreports -A &>/dev/null; then
#             kubectl get policyreports -A -o yaml > ${{ env.REPORTS_DIR }}/kyverno-policyreports.yaml
#           else
#             echo "::warning:: No policyreports found. Skipping Kyverno policy reports export."
#           fi
#           echo "[+] Kyverno policy reports exported."

#       # =======================================================
#       # 7. UPLOAD REPORTS AS ARTIFACTS
#       # =======================================================
#       - name: Upload Security Reports Artifacts
#         uses: actions/upload-artifact@v4
#         with:
#           name: security-reports-${{ github.run_id }}
#           path: ${{ env.REPORTS_DIR }}/
#           retention-days: 7
#           if-no-files-found: ignore

      # =======================================================
      # 8. CLEANUP (Optional - Uninstall App & Security Components)
      # =======================================================
      # - name: Cleanup Kubernetes Resources
      #   if: false # Change to 'true' to enable this cleanup step for testing or a dedicated destroy workflow
      #   run: |
      #     echo "Starting Kubernetes resource cleanup..."
      #     helm uninstall kube-prometheus-stack -n monitoring --wait || true
      #     helm uninstall trivy-operator -n trivy-system --wait || true
      #     helm uninstall falco -n falco --wait || true
      #     helm uninstall kyverno -n kyverno --wait || true
      #     helm uninstall cluster-autoscaler -n kube-system --wait || true
      #     helm uninstall external-dns -n external-dns --wait || true
      #     helm uninstall aws-load-balancer-controller -n kube-system --wait || true
      #     istioctl uninstall --purge -y || true
      #     kubectl delete ns hipster-shop --ignore-not-found --timeout=5m || true
      #     kubectl delete ns external-dns --ignore-not-found --timeout=5m || true
      #     kubectl delete ns falco --ignore-not-found --timeout=5m || true
      #     kubectl delete ns kyverno --ignore-not-found --timeout=5m || true
      #     kubectl delete ns monitoring --ignore-not-found --timeout=5m || true
      #     kubectl delete ns trivy-system --ignore-not-found --timeout=5m || true
      #     echo "Kubernetes components uninstalled."

