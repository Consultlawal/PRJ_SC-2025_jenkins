name: Deploy Security Mesh & Hipster Shop

on:
  workflow_dispatch:
  push:
    branches: [ "main" ]

permissions:
  id-token: write
  contents: read
  security-events: write

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: prj-sc-2025-eks # Use a descriptive cluster name
  REPORTS_DIR: security-reports # Directory for storing all reports

jobs:
  deploy:
    name: Provision and Deploy
    runs-on: ubuntu-latest
    steps:
      # =======================================================
      # 1. SETUP & AUTHENTICATION
      # =======================================================
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.GH_ACTIONS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.0'

      - name: Setup Istioctl CLI
        run: |
          # Download and install istioctl
          curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.0 sh -
          export PATH="$PWD/istio-1.20.0/bin:$PATH"
          echo "$PWD/istio-1.20.0/bin" >> $GITHUB_PATH

      - name: Create Reports Directory
        run: mkdir -p ${{ env.REPORTS_DIR }}

      # =======================================================
      # 2. PROVISION INFRASTRUCTURE (EKS, VPC, IRSA Roles)
      # =======================================================
      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Clean up failed EKS Node Group from state
        id: cleanup_state
        working-directory: ./terraform
        # This step checks for and removes the stuck node group resource from the state file.
        run: |
          echo "Checking for stuck 'aws_eks_node_group.demo' resource in state..."
          if terraform state list | grep "aws_eks_node_group.demo"; then
            echo "Found stuck node group in state. Removing it."
            terraform state rm aws_eks_node_group.demo
            echo "Node group removed from state. Terraform will attempt a clean recreation."
          else
            echo "Node group is clean or successfully created. Skipping state removal."
          fi

      # -------------------------------------------------------------------
      # 2.1. FIRST APPLY: Provision the Cluster and OIDC Provider (OIDC Fix)
      # -------------------------------------------------------------------
      - name: Terraform Apply (Cluster and OIDC Provider)
        id: tf_apply_cluster
        working-directory: ./terraform
        run: terraform apply -target=aws_eks_cluster.demo -target=aws_iam_openid_connect_provider.demo -auto-approve
        env:
          TF_VAR_region: ${{ env.AWS_REGION }}
          TF_VAR_cluster_name: ${{ env.CLUSTER_NAME }}
          
      # -------------------------------------------------------------------
      # 2.2. MANDATORY WAIT AND VALIDATION: CRITICAL FIX for OIDC
      # This step explicitly waits for the EKS OIDC Provider to be ACTIVE in IAM.
      # -------------------------------------------------------------------
      # -------------------------------------------------------------------
      # 2.2. DIAGNOSE AND WAIT: Validate EKS OIDC Provider in IAM
      # This step explicitly fetches the OIDC issuer from Terraform output
      # and then waits for the provider to become active in IAM.
      # -------------------------------------------------------------------
      - name: Diagnose and Wait for EKS OIDC Provider to Become Active in IAM
        id: oidc_validation
        working-directory: ./terraform
        run: |
          echo "Retrieving OIDC Issuer URL and ARN from Terraform outputs..."
          OIDC_ISSUER_URL=$(terraform output -raw eks_oidc_issuer_url)
          OIDC_PROVIDER_ARN=$(terraform output -raw eks_oidc_provider_arn)

          if [ -z "$OIDC_ISSUER_URL" ] || [ -z "$OIDC_PROVIDER_ARN" ]; then
            echo "Error: Terraform did not output EKS OIDC Issuer URL or Provider ARN. OIDC provider might not have been created."
            exit 1
          fi

          echo "EKS OIDC Issuer URL from Terraform: $OIDC_ISSUER_URL"
          echo "EKS OIDC Provider ARN from Terraform: $OIDC_PROVIDER_ARN"

          echo "Waiting for OIDC Identity Provider to become active in IAM using ARN..."
          for i in $(seq 1 30); do # Try for 30 iterations (up to 5 minutes)
            # Check if the OIDC provider exists and is queryable
            IAM_CHECK_OUTPUT=$(aws iam get-open-id-connect-provider --open-id-connect-provider-arn "$OIDC_PROVIDER_ARN" --query "Url" --output text 2>/dev/null)
            
            if [ -n "$IAM_CHECK_OUTPUT" ]; then
              echo "OIDC Identity Provider found in IAM and URL is '$IAM_CHECK_OUTPUT'."
              echo "OIDC provider is active and fully propagated."
              break
            else
              echo "OIDC Identity Provider not yet found/active in IAM or API call failed. Waiting... ($i/30)"
            fi
            sleep 10 # Wait 10 seconds between checks
          done

          if [ -z "$IAM_CHECK_OUTPUT" ]; then
            echo "Error: OIDC Identity Provider did not become active in IAM within the expected time ($OIDC_PROVIDER_ARN)."
            exit 1
          fi
          
      # -------------------------------------------------------------------
      # 2.3. SECOND APPLY: Apply Remaining Resources (IRSA Roles, Node Group, etc.)
      # -------------------------------------------------------------------
      - name: Terraform Apply (IRSA and Remaining Resources)
        id: tf_apply_irsa_and_rest
        working-directory: ./terraform
        run: terraform apply -auto-approve
        env:
          TF_VAR_region: ${{ env.AWS_REGION }}
          TF_VAR_cluster_name: ${{ env.CLUSTER_NAME }}
          
      # =======================================================
      # 3. FETCH TERRAFORM OUTPUTS
      # =======================================================
      - name: Fetch IRSA Role ARNs
        id: fetch_arns
        working-directory: ./terraform
        run: |
          ALB_ARN=$(terraform output -raw alb_controller_role_arn)
          DNS_ARN=$(terraform output -raw external_dns_role_arn)
          AUTOSCALER_ARN=$(terraform output -raw autoscaler_iam_role_arn)
          
          echo "ALB_CONTROLLER_ARN=$ALB_ARN" >> $GITHUB_ENV
          echo "EXTERNAL_DNS_ARN=$DNS_ARN" >> $GITHUB_ENV
          echo "CLUSTER_AUTOSCALER_ARN=$AUTOSCALER_ARN" >> $GITHUB_ENV
        
      # =======================================================
      # 4. PREPARE KUBERNETES CONTEXT
      # =======================================================
      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      # =======================================================
      # 5. SETUP NETWORKING & ACCESS (ALB and ExternalDNS)
      # =======================================================
      - name: Install AWS Load Balancer Controller
        run: |
          helm repo add aws-load-balancer-controller https://aws.github.io/eks-charts
          helm repo update

          helm upgrade --install aws-load-balancer-controller aws-load-balancer-controller/aws-load-balancer-controller \
            --set clusterName=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=true \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.ALB_CONTROLLER_ARN }}" \
            --namespace kube-system --wait
          
      - name: Install ExternalDNS
        run: |
          helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/
          helm repo update

          helm upgrade --install external-dns external-dns/external-dns \
            --set provider=aws \
            --set txtOwnerId=${{ env.CLUSTER_NAME }} \
            --set serviceAccount.create=true \
            --set serviceAccount.name=external-dns \
            --set policy=sync \
            --set aws.zoneType=public \
            --set registry=txt \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${{ env.EXTERNAL_DNS_ARN }}" \
            --namespace external-dns --create-namespace --wait
            
      - name: Install Cluster Autoscaler
        if: ${{ env.CLUSTER_AUTOSCALER_ARN != '' }}
        run: |
          helm repo add autoscaler https://kubernetes.github.io/autoscaler
          helm repo update
          
          helm upgrade --install cluster-autoscaler autoscaler/cluster-autoscaler \
            --namespace kube-system \
            --set 'autoDiscovery.clusterName'=${{ env.CLUSTER_NAME }} \
            --set rbac.create=true \
            --set serviceAccount.create=true \
            --set 'serviceAccount.annotations.eks\.amazonaws\.com/role-arn'="${{ env.CLUSTER_AUTOSCALER_ARN }}" \
            --set awsRegion=${{ env.AWS_REGION }} \
            --wait
          
          echo "Cluster Autoscaler installation complete."
          
      # =======================================================
      # 6. DEPLOY SECURITY MESH COMPONENTS
      # =======================================================
      - name: Install Kyverno (Policy Engine)
        run: |
          helm repo add kyverno https://kyverno.github.io/kyverno/
          helm repo update
          helm upgrade --install kyverno kyverno/kyverno -n kyverno --create-namespace --wait
          echo "Kyverno installation complete."

      - name: Apply Kyverno Policies
        run: |
          if [ -d "k8s-policies/kyverno" ]; then
            kubectl apply -f k8s-policies/kyverno/
            sleep 10
            echo "Kyverno policies applied from k8s-policies/kyverno/"
          else
            echo "::warning:: k8s-policies/kyverno/ directory not found. Skipping Kyverno policy application."
          fi

      - name: Install Falco (Runtime Security Engine)
        run: |
          helm repo add falcosecurity https://falcosecurity.github.io/charts
          helm repo update
          helm upgrade --install falco falcosecurity/falco -n falco --create-namespace --wait
          echo "Falco installation complete."

      - name: Install Trivy Operator (Vulnerability Scanner)
        run: |
          helm repo add aqua https://aquasecurity.github.io/helm-charts
          helm repo update
          helm upgrade --install trivy-operator aqua/trivy-operator -n trivy-system --create-namespace --wait
          echo "Trivy Operator installation complete."
          
      - name: Install Istio Service Mesh (Demo Profile)
        run: |
          istioctl install --set profile=demo -y
          echo "Istio Service Mesh installed with the demo profile."

      - name: Install Prometheus and Grafana (Monitoring Stack)
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update

          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --version 47.6.0 \
            --namespace monitoring --create-namespace \
            -f k8s-policies/monitoring/monitoring-values.yaml
          
          echo "Prometheus and Grafana installation complete."

      # =======================================================
      # 7. DEPLOY APPLICATION (Hipster Shop & Route 53 Ingress)
      # =======================================================
      - name: Deploy Hipster Shop and Ingress
        run: |
          kubectl create ns hipster-shop || true
          
          # CRITICAL: Label the namespace to enable Istio sidecar injection
          kubectl label namespace hipster-shop istio-injection=enabled --overwrite
          
          # Deploy the core application
          kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml -n hipster-shop
          
          # Deploy the Ingress resource
          kubectl apply -f hipster-shop-ingress.yaml -n hipster-shop
          
          echo "Hipster Shop and Ingress deployed. Waiting for readiness..."
          kubectl wait --for=condition=Ready pod -l app=frontend -n hipster-shop --timeout=5m || true

      # =======================================================
      # 8. RUN SECURITY AUDITS & REPORTING
      # =======================================================
      - name: Audit Calico Network Policies
        run: |
          echo "[+] Exporting Calico Network Policies..."
          kubectl get networkpolicies -A -o json > ${{ env.REPORTS_DIR }}/calico-networkpolicies.json || true
        
      - name: Audit Istio Security Configurations
        run: |
          echo "[+] Exporting Istio Security Configurations..."
          kubectl get peerauthentication -A -o json > ${{ env.REPORTS_DIR }}/istio-peerauth.json || true
          kubectl get authorizationpolicy -A -o json > ${{ env.REPORTS_DIR }}/istio-authz.json || true
          
      - name: Run kube-bench (CIS Benchmark)
        run: |
          echo "[+] Running kube-bench CIS Benchmark scan inside the cluster..."
          kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml
          kubectl wait --for=condition=complete job/kube-bench --timeout=10m
          KUBE_BENCH_POD=$(kubectl get pods -l app=kube-bench -o jsonpath='{.items[0].metadata.name}')
          kubectl logs $KUBE_BENCH_POD > ${{ env.REPORTS_DIR }}/kube-bench-report.txt
          kubectl delete -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml
          echo "[+] kube-bench report saved."
        
      - name: Fetch Falco Alerts
        run: |
          echo "[+] Fetching Falco runtime security alerts..."
          kubectl logs -l app.kubernetes.io/name=falco -n falco --tail=1000 > ${{ env.REPORTS_DIR }}/falco-alerts.log || true
          echo "[+] Falco alerts exported."

      - name: Check Trivy Operator Vulnerability Reports
        run: |
          echo "[+] Checking Trivy Operator vulnerability reports..."
          kubectl get vulnerabilityreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-vulnerabilityreports.yaml || true
          kubectl get configauditreports -A -o yaml > ${{ env.REPORTS_DIR }}/trivy-operator-configauditreports.yaml || true
          echo "[+] Trivy Operator reports exported."

      - name: Check Kyverno Policy Reports
        run: |
          echo "[+] Checking Kyverno policy reports..."
          kubectl get policyreports -A -o yaml > ${{ env.REPORTS_DIR }}/kyverno-policyreports.yaml || true
          echo "[+] Kyverno policy reports exported."

      # =======================================================
      # 9. UPLOAD REPORTS AS ARTIFACTS
      # =======================================================
      - name: Upload Security Reports Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: security-reports-${{ github.sha }}
          path: ${{ env.REPORTS_DIR }}/
          retention-days: 7
          if-no-files-found: ignore


      # =======================================================
      # 10. CLEANUP (Terraform Destroy)
      # =======================================================
      - name: Terraform Destroy
        if: always() 
        working-directory: ./terraform
        run: |
          istioctl uninstall -y || true
          # NOTE: Uncomment the line below to enable auto-destruction of your AWS resources.
          # terraform destroy -auto-approve
          terraform plan -destroy
        env:
          TF_VAR_region: ${{ env.AWS_REGION }}
          TF_VAR_cluster_name: ${{ env.CLUSTER_NAME }}

# ```http://googleusercontent.com/image_generation_content/0


# Please do the following:

# 1.  **Go to your GitHub repository.**
# 2.  **Click on "Actions."**
# 3.  **Select the latest workflow run.**
# 4.  **Click on the "deploy" job.**
# 5.  **Expand all the steps** (especially steps 2.1, 2.2, and 2.3) and **copy the entire log output as plain text.** Do not provide another screenshot if the text is garbled.

# **Specifically, I need to see the output of:**

# * **"2.1. FIRST APPLY: Provision the Cluster and OIDC Provider (OIDC Fix)"**: Did this succeed?
# * **"2.2. MANDATORY WAIT AND VALIDATION: CRITICAL FIX for OIDC"**: Did this step output "OIDC Identity Provider is active and fully propagated" and complete successfully without `exit 1`?
# * **"2.3. SECOND APPLY: Apply Remaining Resources (IRSA Roles, Node Group, etc.)"**: This is where we expect the IRSA role creation to finally succeed or clearly show a *new* error.

# Once I have the full, plain text logs, we can pinpoint exactly why the OIDC error is still occurring or if a new problem has emerged.